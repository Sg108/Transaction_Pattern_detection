{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "974fbcc2-8ae9-49b1-8ee7-6169605b288b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, regexp_replace\n",
    "from pyspark.sql.types import IntegerType\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import time\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "OFFSET=0\n",
    "OFFSET_FILE=\"Offset/offset.csv\"\n",
    "BUCKET_NAME = \"databricksbucket101\"\n",
    "FILE = \"datafiles/transactions.csv\"\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        s3_client = boto3.client('s3')\n",
    "        s3_client.head_object(Bucket=BUCKET_NAME, Key=OFFSET_FILE)\n",
    "\n",
    "        print(f\"File found at s3://{BUCKET_NAME}/{OFFSET_FILE}. Reading file...\")\n",
    "\n",
    "        s3_path = f\"s3://{BUCKET_NAME}/{OFFSET_FILE}\"\n",
    "        response = s3_client.get_object(Bucket=BUCKET_NAME, Key=OFFSET_FILE)\n",
    "\n",
    "        csv_object = response['Body']\n",
    "\n",
    "        df_offset = pd.read_csv(csv_object)\n",
    "        OFFSET=df_offset.iloc[0]['offset']\n",
    "\n",
    "    except ClientError as e:\n",
    "      \n",
    "        if e.response['Error']['Code'] == '404':\n",
    "            print(f\"File not found at s3://{BUCKET_NAME}/{OFFSET_FILE}.\")\n",
    "            OFFSET=0\n",
    "               \n",
    "        else:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "\n",
    "    try:\n",
    "\n",
    "        # Get the object from S3\n",
    "        response = s3_client.get_object(Bucket=BUCKET_NAME, Key=FILE)\n",
    "\n",
    "        # The actual data is in the 'Body' of the response, which is a streaming object\n",
    "        csv_object = response['Body']\n",
    "\n",
    "        # Read the streaming object directly into pandas\n",
    "        df_pd = pd.read_csv(csv_object)\n",
    "        print(df_pd.shape[0],OFFSET)\n",
    "        print(\"Successfully read CSV from S3 using boto3.\")\n",
    "        \n",
    "        df=pd.DataFrame(df_pd.iloc[OFFSET:OFFSET+10000])\n",
    "        for col in df.select_dtypes(include='object'):\n",
    "            df[col] = df[col].str.replace(\"'\", \"\")\n",
    "        new_offset=OFFSET+10000\n",
    "        offset_data={\"offset\":[new_offset]}\n",
    "        df_offset = pd.DataFrame(offset_data)\n",
    "        csv_buffer1 = StringIO()\n",
    "        df.to_csv(csv_buffer1, index=False)\n",
    "        s3_client.put_object(Bucket=BUCKET_NAME, Key=f\"landing_zone/transactions_{new_offset}.csv\", Body=csv_buffer1.getvalue())\n",
    "        csv_buffer2 = StringIO()\n",
    "        df_offset.to_csv(csv_buffer2, index=False)\n",
    "        s3_client.put_object(Bucket=BUCKET_NAME,Key=\"Offset/offset.csv\",Body=csv_buffer2.getvalue())\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    if df_pd.shape[0]<=int(OFFSET):\n",
    "        print(\"read the whole transactions file\")\n",
    "        break\n",
    "    time.sleep(5)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "chunk_generator",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
