{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "32e4f1ea-cc04-4c70-bd85-eed8787633ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as fn\n",
    "from pyspark.sql.window import Window\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import boto3\n",
    "Ystarttime=datetime.now()\n",
    "def process_and_log(batchdf, batchId):\n",
    "  batchdf.write.format(\"delta\").mode(\"append\").save(\"s3://databricksbucket101/delta/\")\n",
    "  df_custImp=spark.read.format(\"csv\").options(header=\"true\", inferSchema=\"true\") \\\n",
    "  .load(\"s3://databricksbucket101/datafiles/CustomerImportance.csv\")\n",
    "  df_custImp=df_custImp.withColumn(\"Target\",fn.regexp_replace(df_custImp.Target, \"'\", \"\")) \\\n",
    "  .withColumn(\"Source\",fn.regexp_replace(df_custImp.Source, \"'\", \"\"))\n",
    "\n",
    "  w = Window.partitionBy(\"Target\").orderBy(\"weight\")\n",
    "  df_custImp_flt = df_custImp.withColumn(\"pct_rank\", fn.percent_rank().over(w)) \\\n",
    "     .withColumn(\"percentile\", fn.col(\"pct_rank\") * 100).filter(fn.col(\"percentile\") <= 10)\n",
    "  df_tnxs = spark.read.format(\"delta\").load(\"s3://databricksbucket101/delta/\")\n",
    "  df_tnxs_total = df_tnxs.groupby(\"customer\").agg(fn.count(\"*\").alias(\"total_customer_txns\"))\n",
    "  df_tnxs_cnts = df_tnxs.groupby(\"customer\",\"merchant\").agg(fn.count(\"*\").alias(\"txn_cnt\"),fn.avg(\"amount\").alias (\"avg_amt\"))\n",
    "  w = Window.partitionBy(\"merchant\").orderBy(\"txn_cnt\")\n",
    "  df_tnxs_flt = df_tnxs_cnts.withColumn(\"pct_rank\", fn.percent_rank().over(w)) \\\n",
    "     .withColumn(\"percentile\",fn.col(\"pct_rank\") * 100).filter(fn.col(\"percentile\") > 90)\n",
    "  df_patt1=df_tnxs_flt.join(df_tnxs_total, \"customer\").join(df_custImp_flt,((df_tnxs_flt[\"customer\"]==df_custImp_flt  [\"Source\"]) & (df_tnxs_flt[\"merchant\"]==df_custImp_flt[\"Target\"])))\n",
    "  patt1_time=datetime.now()\n",
    "  df_patt1=df_patt1.select(\"customer\",\"merchant\").withColumn(\"patternId\",fn.lit(\"1\")).withColumn(\"ActionType\",fn.lit  (\"UPGRADE\")).withColumn(\"YStarttime\",fn.lit(Ystarttime)).withColumn(\"detectiontime\",fn.lit(patt1_time))\n",
    "  pandas_df = df_patt1.toPandas()\n",
    "\n",
    "\n",
    "  df_patt2=df_tnxs_cnts.filter((fn.col(\"avg_amt\")<23) & (fn.col(\"txn_cnt\")>=80))\n",
    "  patt2_time=datetime.now()\n",
    "  df_patt2=df_patt2.select(\"customer\",\"merchant\").withColumn(\"patternId\",fn.lit(\"2\")).withColumn(\"ActionType\",fn.lit  (\"CHILD\")).withColumn(\"YStarttime\",fn.lit(Ystarttime)).withColumn(\"detectiontime\",fn.lit(patt2_time))\n",
    "  pandas_df = pd.concat([pandas_df, df_patt2.toPandas()], ignore_index=True)\n",
    "\n",
    "\n",
    "  df_male=df_tnxs.filter(fn.col(\"gender\")==\"M\").groupby(\"merchant\").agg(fn.count(\"*\").alias(\"male_cnt\"))\n",
    "  df_female=df_tnxs.filter(fn.col(\"gender\")==\"F\").groupby(\"merchant\").agg(fn.count(\"*\").alias(\"female_cnt\"))\n",
    "  df_male_female=df_male.join(df_female, \"merchant\")\n",
    "  df_patt3=df_male_female.filter((fn.col(\"male_cnt\")>fn.col(\"female_cnt\"))&(fn.col(\"female_cnt\")>100))\n",
    "  patt3_time=datetime.now()\n",
    "  df_patt3=df_patt3.select(\"merchant\").withColumn(\"customer\",fn.lit(\"\")).withColumn(\"patternId\",fn.lit(\"3\")). withColumn(\"ActionType\",fn.lit(\"DEI-NEEDED\")).withColumn(\"YStarttime\",fn.lit(Ystarttime)).withColumn(\"detectiontime\",  fn.lit(patt3_time))\n",
    "  pandas_df = pd.concat([pandas_df, df_patt3.toPandas()], ignore_index=True)\n",
    "  #display(pandas_df)\n",
    "\n",
    "  try:\n",
    "      s3_client = boto3.client('s3')\n",
    "      i=0\n",
    "      print(pandas_df.shape[0])\n",
    "      while i<pandas_df.shape[0]:\n",
    "        pd_chunk=pd.DataFrame(pandas_df.iloc[i:i+50])\n",
    "        csv_buffer = StringIO()\n",
    "        pd_chunk.to_csv(csv_buffer, index=False)\n",
    "        s3_client.put_object(Bucket=\"databricksbucket101\", Key=f\"detections/detection_{i}.csv\",Body=csv_buffer.getvalue())\n",
    "        i=i+50\n",
    "        print(i)\n",
    "  except Exception as e:\n",
    "      print(e)\n",
    "\n",
    "stream = (\n",
    "  spark.readStream\n",
    "       .format(\"cloudFiles\")\n",
    "       .option(\"cloudFiles.format\", \"csv\")\n",
    "       .option(\"cloudFiles.schemaLocation\", \"s3://databricksbucket101/schema/\")\n",
    "       .load(\"s3://databricksbucket101/landing_zone/\")\n",
    ")\n",
    "\n",
    "query = (\n",
    "  stream.writeStream\n",
    "        .foreachBatch(process_and_log)\n",
    "        .option(\"checkpointLocation\", \"s3://databricksbucket101/checkpoint/\")\n",
    "        .trigger(availableNow=True)\n",
    "        .start()\n",
    ")\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9fa05d23-3d01-43a8-91a8-116f1f82ad73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "pattern_detections",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
